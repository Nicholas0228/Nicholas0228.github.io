---
title: "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples"
collection: publications
category: conferences
permalink: /publication/2024-02-17-adversarial-example-painting-imitation
excerpt: 'This paper presents a framework to prevent painting imitation from diffusion models using adversarial examples.'
date: 2023-10-27
venue: 'ICML 2023 (Oral)'
paperurl: 'https://proceedings.mlr.press/v202/liang23g/liang23g.pdf'
citation: 'Liang C*, Wu X*, Hua Y, et al. (2023). "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples." <i>ICML 2023 (Oral)</i>. (Co-First Author)'
---
 In this paper, we established a framework to craft adversarial watermarks to protect against unauthorized diffusion model-based artwork mimicry by theoretically modeling adversarial attacks on diffusion models. This work has been packaged into an open-source project called *Mist:*

---

**Mist: Watermark Against Unauthorized Diffusion-Based Artwork Mimicking**

*Oct. 2022 â€” Present*

**Homepage**: [https://psyker-team.github.io/index_en.html](https://psyker-team.github.io/index_en.html)

- The only open-source watermarking tool to combat unauthorized art mimicry by Stable Diffusion models.
- **GitHub Stars**: 634 (360 + 274)
- **Media**: 16k reposts, 20k likes
